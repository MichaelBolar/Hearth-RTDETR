{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ee072-4822-4c62-aaa6-ce95a82a6480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "#Import RTDETR\n",
    "from transformers import RTDetrV2ForObjectDetection, RTDetrImageProcessor\n",
    "\n",
    "token = \"Insert Hugging face token here\"\n",
    "\n",
    "#Load pretrained image processor and model\n",
    "image_processor = RTDetrImageProcessor.from_pretrained(\n",
    "    \"PekingU/rtdetr_v2_r50vd\",\n",
    "    token=token\n",
    ")\n",
    "\n",
    "model = RTDetrV2ForObjectDetection.from_pretrained(\n",
    "    \"PekingU/rtdetr_v2_r50vd\",\n",
    "    token=token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f6ef03d-8ad6-47d1-8b35-58087d8613f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find image and load image an an input to our image processor\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = image_processor(images=image, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc38b1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import/Get dataset to run inference on (Must be PIL Images,Tensors, or Arrays)\n",
    "IMAGE_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif', '.webp'}\n",
    "\n",
    "#Get images from a image folder\n",
    "def get_images_from_folder(folder_path):\n",
    "    folder = Path(folder_path)\n",
    "    \n",
    "    if not folder.exists():\n",
    "        raise FileNotFoundError(f\"Folder not found: {folder_path}\")\n",
    "    \n",
    "    # Collect all image files\n",
    "    image_files = []\n",
    "    for file_path in folder.iterdir():\n",
    "        if file_path.suffix.lower() in IMAGE_EXTENSIONS:\n",
    "            image_files.append(file_path)\n",
    "    \n",
    "    return sorted(image_files)  # Sort for consistent order\n",
    "\n",
    "folder_paths = get_images_from_folder(r\"C:\\Users\\Bo_jr\\Documents\\Datasets\\SampleDataset\")\n",
    "\n",
    "for path in folder_paths:\n",
    "    image = Image.open(path).convert('RGB')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9724516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference function that takes in an image, image processor, and model and returns the results\n",
    "def inference(imgs, img_proc, model):\n",
    "    test_imgs = imgs\n",
    "    #model.eval()\n",
    "    with torch.no_grad():\n",
    "        #The image processor packs the images into a dictionary\n",
    "        trans_img = img_proc(images=imgs, return_tensors='pt')\n",
    "        \n",
    "        #The images can be unpacked via the ** operator\n",
    "        results = model(**trans_img)\n",
    "\n",
    "        return results\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5503ffb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vase: 0.64 [15.18, 0.61, 40.28, 63.5]\n",
      "broccoli: 0.70 [0.02, 10.58, 64.02, 56.08]\n",
      "diningtable: 0.52 [-0.0, 0.1, 64.0, 63.71]\n",
      "person: 0.64 [8.69, 16.39, 27.23, 47.81]\n",
      "orange: 0.63 [6.22, 36.39, 24.7, 49.43]\n",
      "apple: 0.61 [31.09, 48.29, 45.0, 61.66]\n",
      "orange: 0.57 [22.42, 31.72, 33.13, 42.16]\n",
      "orange: 0.55 [-0.0, 20.42, 33.34, 52.17]\n",
      "orange: 0.54 [0.01, 22.79, 9.07, 36.14]\n",
      "orange: 0.52 [4.27, 28.99, 18.28, 42.06]\n",
      "orange: 0.51 [-0.0, 35.93, 5.1, 47.92]\n",
      "orange: 0.50 [5.18, 28.94, 18.26, 38.99]\n",
      "person: 0.76 [11.45, 5.31, 30.49, 63.82]\n",
      "vase: 0.53 [5.38, 8.69, 56.69, 60.76]\n",
      "keyboard: 0.79 [0.03, 29.24, 64.03, 49.71]\n",
      "keyboard: 0.55 [-0.0, 0.08, 64.0, 28.69]\n",
      "person: 0.76 [0.07, 0.06, 61.3, 63.91]\n",
      "sports ball: 0.63 [32.83, 25.87, 49.06, 50.14]\n",
      "person: 0.78 [-0.03, 0.05, 63.97, 63.8]\n",
      "person: 0.82 [17.1, 14.96, 33.42, 63.95]\n",
      "person: 0.81 [8.82, 18.94, 21.39, 63.93]\n",
      "person: 0.70 [0.03, 17.58, 9.82, 63.8]\n",
      "person: 0.63 [53.1, 17.86, 64.0, 63.91]\n",
      "person: 0.57 [35.0, 14.76, 53.67, 63.79]\n",
      "person: 0.93 [0.11, 0.08, 52.15, 63.75]\n",
      "bottle: 0.61 [12.6, -0.04, 53.97, 63.95]\n",
      "vase: 0.50 [12.6, -0.04, 53.97, 63.95]\n",
      "pizza: 0.63 [0.01, 4.9, 64.01, 63.64]\n",
      "kite: 0.55 [23.04, 0.04, 58.62, 63.73]\n",
      "frisbee: 0.88 [15.34, 44.67, 37.32, 62.86]\n",
      "person: 0.67 [0.1, -0.03, 55.26, 63.93]\n",
      "person: 0.67 [0.0, -0.12, 44.88, 63.87]\n",
      "cell phone: 0.79 [1.78, 32.95, 59.31, 48.96]\n",
      "remote: 0.52 [1.76, 6.79, 59.2, 24.98]\n",
      "bird: 0.67 [16.53, 8.22, 56.02, 39.85]\n",
      "person: 0.69 [0.02, 7.5, 64.02, 63.71]\n"
     ]
    }
   ],
   "source": [
    "#Get images from sample dataset and run inference\n",
    "outputs = []\n",
    "\n",
    "folder_paths = get_images_from_folder(r\"C:\\Users\\Bo_jr\\Documents\\Datasets\\SampleDataset\")\n",
    "for path in folder_paths:\n",
    "    image = Image.open(path).convert('RGB')\n",
    "    output = inference(image, image_processor, model)\n",
    "    #Converts the raw output into final bounding boxes in (top_left_x, top_left_y, bottom_right_x, bottom_right_y) and append the output to the outputs list\n",
    "    outputs.append(image_processor.post_process_object_detection(output, target_sizes=torch.tensor([(image.height, image.width)]), threshold=0.5))\n",
    "\n",
    "#Unpack the outputs and print the results\n",
    "for result in outputs:\n",
    "    for res in result:\n",
    "        #Simultaneously stores each detection result into their respective variables\n",
    "        for score, label, box in zip(res[\"scores\"], res[\"labels\"], res[\"boxes\"]):\n",
    "            if box.numel() != 0:\n",
    "                #Convert the tensor values to scalars\n",
    "                score, label = score.item(), label.item()\n",
    "                #Convert box values to floats\n",
    "                box = [round(i, 2) for i in box.tolist()]\n",
    "                print(f\"{model.config.id2label[label]}: {score:.2f} {box}\")\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4dba447f-bb1b-4338-a636-75e13638fa0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: 0.96 [13.71, 54.12, 317.53, 472.65]\n",
      "cat: 0.95 [343.73, 23.68, 640.28, 373.05]\n",
      "sofa: 0.94 [0.2, 1.32, 640.17, 474.38]\n",
      "remote: 0.93 [40.6, 73.21, 175.74, 118.33]\n",
      "remote: 0.89 [333.51, 76.79, 370.17, 188.13]\n"
     ]
    }
   ],
   "source": [
    "#Run inference with the model and collect the results\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "#Converts the raw output into final bounding boxes in (top_left_x, top_left_y, bottom_right_x, bottom_right_y) format\n",
    "results = image_processor.post_process_object_detection(outputs, target_sizes=torch.tensor([(image.height, image.width)]), threshold=0.5)\n",
    "\n",
    "\n",
    "\n",
    "for result in results:\n",
    "    for score, label_id, box in zip(result[\"scores\"], result[\"labels\"], result[\"boxes\"]):\n",
    "        score, label = score.item(), label_id.item()\n",
    "        box = [round(i, 2) for i in box.tolist()]\n",
    "        print(f\"{model.config.id2label[label]}: {score:.2f} {box}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d717b31-09fc-4cbd-ad72-072defd15a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been saved\n"
     ]
    }
   ],
   "source": [
    "#Draw Bounding Boxes using OpenCV\n",
    "import cv2\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "\n",
    "#Read img Url\n",
    "urlresp = urllib.request.urlopen(url)\n",
    "image_data = np.asarray(bytearray(urlresp.read()), dtype='uint8')\n",
    "img = cv2.imdecode(image_data, cv2.IMREAD_COLOR)\n",
    "\n",
    "boxes = result['boxes']\n",
    "scores = result['scores']\n",
    "\n",
    "classes = ['Cat', 'Remote', 'Sofa']\n",
    "\n",
    "scores = scores.softmax(-1)\n",
    "conf, cls = scores.max(-1)\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "for i in range(len(boxes)):\n",
    "    if conf.item() > threshold:\n",
    "        box = boxes[0]\n",
    "\n",
    "    box = boxes[i]\n",
    "    x1, y1, x2, y2 = box.int().tolist()\n",
    "\n",
    "    label = f\"{classes[cls.item()]}: {conf.item():.2f}\"\n",
    "\n",
    "    #Draw rectangles around objects\n",
    "    cv2.rectangle(img, (x1,y1), (x2,y2), (0,255,0), 2)\n",
    "    cv2.putText(img, label, (x1, y1 - 5),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)\n",
    "    \n",
    "cv2.imwrite('prediction_results.jpg', img)\n",
    "print('Results have been saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a506e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9545465111732483 0\n",
      "0.7257509231567383 71\n",
      "0.9597740173339844 0\n",
      "0.7301617860794067 71\n",
      "0.5973457098007202 39\n",
      "0.5579811930656433 27\n",
      "0.5157375335693359 56\n",
      "0.9659205079078674 0\n",
      "0.6817017793655396 71\n",
      "0.5172863006591797 79\n",
      "0.9665103554725647 0\n",
      "0.6910728216171265 71\n",
      "0.6284246444702148 56\n",
      "0.96346515417099 0\n",
      "0.6163796782493591 71\n",
      "0.5151272416114807 39\n",
      "0.9647098183631897 0\n",
      "0.7187235951423645 71\n",
      "0.9568052291870117 0\n",
      "0.5888808369636536 39\n",
      "0.5303278565406799 71\n"
     ]
    }
   ],
   "source": [
    "#Open webcam\n",
    "rtv = cv2.VideoCapture(0)\n",
    "\n",
    "#Check if webcam is opened\n",
    "if not rtv.isOpened():\n",
    "    print(\"Error: Could not open webcam\")\n",
    "    exit()\n",
    "\n",
    "#Read frames from webcam\n",
    "while True:\n",
    "    ret, frame = rtv.read()\n",
    "    \n",
    "    #Display frames\n",
    "    cv2.imshow('Webcam', frame)\n",
    "\n",
    "    #Run inference with the model\n",
    "    output = inference(frame, image_processor, model)\n",
    "\n",
    "    #Converts the raw output into final bounding boxes in (top_left_x, top_left_y, bottom_right_x, bottom_right_y) format\n",
    "    results = image_processor.post_process_object_detection(output, target_sizes=torch.tensor([(frame.shape[1], frame.shape[0])]), threshold=0.5)\n",
    "\n",
    "\n",
    "    for result in results:\n",
    "        #Unpack the each list simultaneously and pair each result with one another\n",
    "        for score, label, box in zip(result['scores'], result['labels'], result['boxes']):\n",
    "            #Convert each tensor value into a scalar\n",
    "            score, label = score.item(), label.item()\n",
    "            \n",
    "\n",
    "\n",
    "    #Press 'q' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "#Release webcam\n",
    "#Always release the webcam after opening it\n",
    "rtv.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "583b607e-f159-48d1-a364-ba041322c142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Function that returns the box coordinates and the scores from the results\\ndef get_boxes_n_scores(results):\\n    for result in results:\\n        boxes = result['boxes']\\n        boxes.int()\\n        scores = result['scores']\\n        return boxes, scores\\n\\nget_boxes_n_scores(results)\\n#Convert boxes and scores to int values\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Function that returns the box coordinates and the scores from the results\n",
    "def get_boxes_n_scores(results):\n",
    "    for result in results:\n",
    "        boxes = result['boxes']\n",
    "        boxes.int()\n",
    "        scores = result['scores']\n",
    "        return boxes, scores\n",
    "\n",
    "get_boxes_n_scores(results)\n",
    "#Convert boxes and scores to int values\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1cfcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "   #Working example code\n",
    "   \n",
    "    for result in results:\n",
    "        for score, label, box in zip(result[\"scores\"], result[\"labels\"], result[\"boxes\"]):\n",
    "            if box.numel() != 0:\n",
    "                score, label = score.item(), label.item()\n",
    "                box = [round(i, 2) for i in box.tolist()]\n",
    "                print(f\"{model.config.id2label[label]}: {score:.2f} {box}\")\n",
    "    \n",
    "    #Draw bounding boxes\n",
    "    for result in results:\n",
    "        for score, label, box in zip(result[\"scores\"], result[\"labels\"], result[\"boxes\"]):\n",
    "            if box.numel() != 0:\n",
    "                box = con2xy(box)\n",
    "                x1, y1, x2, y2 = box.int().tolist()\n",
    "                cv2.rectangle(frame, (x1,y1), (x2,y2), (0,255,0), 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
