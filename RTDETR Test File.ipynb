{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ee072-4822-4c62-aaa6-ce95a82a6480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "#Import RTDETR\n",
    "from transformers import RTDetrV2ForObjectDetection, RTDetrImageProcessor\n",
    "\n",
    "token = \"Insert Hugging Face Token Here\"\n",
    "\n",
    "#Load pretrained image processor and model\n",
    "image_processor = RTDetrImageProcessor.from_pretrained(\n",
    "    \"PekingU/rtdetr_v2_r50vd\",\n",
    "    token=token\n",
    ")\n",
    "\n",
    "model = RTDetrV2ForObjectDetection.from_pretrained(\n",
    "    \"PekingU/rtdetr_v2_r50vd\",\n",
    "    token=token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9724516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference function that takes in an image, image processor, and model and returns the results\n",
    "def inference(imgs, img_proc, model):\n",
    "    test_imgs = imgs\n",
    "    #model.eval()\n",
    "    with torch.no_grad():\n",
    "        #The image processor packs the images into a dictionary\n",
    "        trans_img = img_proc(images=imgs, return_tensors='pt')\n",
    "        \n",
    "        #The images can be unpacked via the ** operator\n",
    "        results = model(**trans_img)\n",
    "\n",
    "        return results\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6ac2902",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import/Get dataset to run inference on (Must be PIL Images,Tensors, or Arrays)\n",
    "IMAGE_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif', '.webp'}\n",
    "\n",
    "#Get images from a image folder\n",
    "def get_images_from_folder(folder_path):\n",
    "    folder = Path(folder_path)\n",
    "    \n",
    "    if not folder.exists():\n",
    "        raise FileNotFoundError(f\"Folder not found: {folder_path}\")\n",
    "    \n",
    "    # Collect all image files\n",
    "    image_files = []\n",
    "    for file_path in folder.iterdir():\n",
    "        if file_path.suffix.lower() in IMAGE_EXTENSIONS:\n",
    "            image_files.append(file_path)\n",
    "    \n",
    "    return sorted(image_files)  # Sort for consistent order\n",
    "\n",
    "folder_paths = get_images_from_folder(r\"C:\\Users\\Bo_jr\\Documents\\Datasets\\SampleDataset\")\n",
    "\n",
    "for path in folder_paths:\n",
    "    image = Image.open(path).convert('RGB')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5503ffb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vase: 63.55% [15.18, 0.61, 40.28, 63.5]\n",
      "broccoli: 70.25% [0.02, 10.58, 64.02, 56.08]\n",
      "diningtable: 52.46% [-0.0, 0.1, 64.0, 63.71]\n",
      "person: 64.26% [8.69, 16.39, 27.23, 47.81]\n",
      "orange: 62.54% [6.22, 36.39, 24.7, 49.43]\n",
      "apple: 60.62% [31.09, 48.29, 45.0, 61.66]\n",
      "orange: 57.01% [22.42, 31.72, 33.13, 42.16]\n",
      "orange: 55.38% [-0.0, 20.42, 33.34, 52.17]\n",
      "orange: 53.97% [0.01, 22.79, 9.07, 36.14]\n",
      "orange: 51.59% [4.27, 28.99, 18.28, 42.06]\n",
      "orange: 50.90% [-0.0, 35.93, 5.1, 47.92]\n",
      "orange: 50.45% [5.18, 28.94, 18.26, 38.99]\n",
      "person: 75.53% [11.45, 5.31, 30.49, 63.82]\n",
      "vase: 53.16% [5.38, 8.69, 56.69, 60.76]\n",
      "keyboard: 78.52% [0.03, 29.24, 64.03, 49.71]\n",
      "keyboard: 55.41% [-0.0, 0.08, 64.0, 28.69]\n",
      "person: 76.21% [0.07, 0.06, 61.3, 63.91]\n",
      "sports ball: 62.50% [32.83, 25.87, 49.06, 50.14]\n",
      "person: 78.03% [-0.03, 0.05, 63.97, 63.8]\n",
      "person: 82.41% [17.1, 14.96, 33.42, 63.95]\n",
      "person: 81.12% [8.82, 18.94, 21.39, 63.93]\n",
      "person: 69.76% [0.03, 17.58, 9.82, 63.8]\n",
      "person: 63.36% [53.1, 17.86, 64.0, 63.91]\n",
      "person: 57.00% [35.0, 14.76, 53.67, 63.79]\n",
      "person: 92.81% [0.11, 0.08, 52.15, 63.75]\n",
      "bottle: 60.87% [12.6, -0.04, 53.97, 63.95]\n",
      "vase: 50.26% [12.6, -0.04, 53.97, 63.95]\n",
      "pizza: 62.76% [0.01, 4.9, 64.01, 63.64]\n",
      "kite: 55.30% [23.04, 0.04, 58.62, 63.73]\n",
      "frisbee: 87.70% [15.34, 44.67, 37.32, 62.86]\n",
      "person: 67.43% [0.1, -0.03, 55.26, 63.93]\n",
      "person: 66.78% [0.0, -0.12, 44.88, 63.87]\n",
      "cell phone: 78.57% [1.78, 32.95, 59.31, 48.96]\n",
      "remote: 52.30% [1.76, 6.79, 59.2, 24.98]\n",
      "bird: 67.06% [16.53, 8.22, 56.02, 39.85]\n",
      "person: 69.27% [0.02, 7.5, 64.02, 63.71]\n"
     ]
    }
   ],
   "source": [
    "#Test model quality\n",
    "\n",
    "#Get images from sample dataset and run inference\n",
    "outputs = []\n",
    "\n",
    "folder_paths = get_images_from_folder(r\"C:\\Users\\Bo_jr\\Documents\\Datasets\\SampleDataset\")\n",
    "for path in folder_paths:\n",
    "    image = Image.open(path).convert('RGB')\n",
    "    output = inference(image, image_processor, model)\n",
    "    #Converts the raw output into final bounding boxes in (top_left_x, top_left_y, bottom_right_x, bottom_right_y) and append the output to the outputs list\n",
    "    outputs.append(image_processor.post_process_object_detection(output, target_sizes=torch.tensor([(image.height, image.width)]), threshold=0.5))\n",
    "\n",
    "#Unpack the outputs and print the results\n",
    "for result in outputs:\n",
    "    for res in result:\n",
    "        #Simultaneously stores each detection result into their respective variables\n",
    "        for score, label, box in zip(res[\"scores\"], res[\"labels\"], res[\"boxes\"]):\n",
    "            if box.numel() != 0:\n",
    "                #Convert the tensor values to scalars\n",
    "                score, label = score.item() * 100, label.item()\n",
    "                \n",
    "                #Convert box values to floats\n",
    "                box = [round(i, 2) for i in box.tolist()]\n",
    "                print(f\"{model.config.id2label[label]}: {score:.2f}% {box}\")\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a506e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person: 97.28 [0, 63, 480, 633]\n",
      "sofa: 75.28 [0, 239, 307, 580]\n",
      "person: 96.69 [0, 113, 480, 634]\n",
      "sofa: 64.81 [0, 275, 281, 555]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "\n",
    "#Open webcam\n",
    "rtv = cv2.VideoCapture(0)\n",
    "\n",
    "#Check if webcam is opened\n",
    "if not rtv.isOpened():\n",
    "    print(\"Error: Could not open webcam\")\n",
    "    exit()\n",
    "\n",
    "#Read frames from webcam\n",
    "while True:\n",
    "    ret, frame = rtv.read()\n",
    "        \n",
    "\n",
    "    #Run inference with the model\n",
    "    output = inference(frame, image_processor, model)\n",
    "\n",
    "    #Converts the raw output into final bounding boxes in (top_left_x, top_left_y, bottom_right_x, bottom_right_y) format\n",
    "    results = image_processor.post_process_object_detection(output, target_sizes=torch.tensor([(frame.shape[1], frame.shape[0])]), threshold=0.5)\n",
    "\n",
    "\n",
    "    for result in results:\n",
    "        #Unpack the each list simultaneously and pair each result with one another\n",
    "        for score, label, box in zip(result['scores'], result['labels'], result['boxes']):\n",
    "            #Convert each tensor value into a scalar\n",
    "            score, label = score.item() * 100, label.item()\n",
    "            box = [round(i, 2) for i in box.int().tolist()]\n",
    "            #Print label: Confidence score, and Bounding Box Coordinates\n",
    "            print(f\"{model.config.id2label[label]}: {score:.2f} {box}\")\n",
    "\n",
    "            #Add Bounding Boxes to live video feed\n",
    "            label = model.config.id2label[label]\n",
    "            threshold = 0.5\n",
    "    \n",
    "            for i in range(len(result[\"boxes\"])):\n",
    "                #If the confidence score is above the threshold, draw the bounding box\n",
    "                if score > threshold:\n",
    "                    x1, y1, x2, y2 = box\n",
    "\n",
    "                #Draw rectangles around objects\n",
    "                cv2.rectangle(frame, (x1,y1), (x2,y2), (0,255,0), 2)\n",
    "                cv2.putText(frame, label, (x1, y1 - 5),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)\n",
    "                \n",
    "                #Display camera feed\n",
    "                cv2.imshow('Webcam', frame)\n",
    "\n",
    "    #Press 'q' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "#Release webcam\n",
    "#Always release the webcam after opening it\n",
    "rtv.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
